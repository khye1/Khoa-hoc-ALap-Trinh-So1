import streamlit as st
from openai import OpenAI
import os

# ======================
# H√†m ƒë·ªçc file vƒÉn b·∫£n
# ======================
def rfile(name_file):
    try:
        with open(name_file, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        st.error(f"Kh√¥ng t√¨m th·∫•y file: `{name_file}`")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file `{name_file}`: {e}")
        st.stop()

# ======================
# C·∫•u h√¨nh trang
# ======================
st.set_page_config(page_title="Tr·ª£ l√Ω AI", page_icon="üáÆüá±", layout="centered")

# ======================
# Hi·ªÉn th·ªã logo & ti√™u ƒë·ªÅ
# ======================
try:
    col1, col2, col3 = st.columns([3, 2, 3])
    with col2:
        st.image("israel-flag.png", use_container_width=True)
except:
    pass

title_content = rfile("00.xinchao.txt")
st.markdown(
    f"""<h1 style="text-align: center; font-size: 24px; margin-bottom: 30px;">{title_content}</h1>""",
    unsafe_allow_html=True
)

# ======================
# Ki·ªÉm tra API Key
# ======================
if "OPENAI_API_KEY" not in st.secrets:
    st.error("**L·ªói c·∫•u h√¨nh**: Kh√¥ng t√¨m th·∫•y `OPENAI_API_KEY` trong `.streamlit/secrets.toml`.")
    st.info("H√£y t·∫°o file `.streamlit/secrets.toml` v·ªõi n·ªôi dung:\n\n```toml\nOPENAI_API_KEY = \"sk-...\"\n```")
    st.stop()

openai_api_key = st.secrets["OPENAI_API_KEY"].strip()
if not openai_api_key:
    st.error("**API Key r·ªóng**: Vui l√≤ng ki·ªÉm tra `OPENAI_API_KEY` trong `secrets.toml`.")
    st.stop()

# ======================
# Kh·ªüi t·∫°o OpenAI Client
# ======================
try:
    client = OpenAI(api_key=openai_api_key)
except Exception as e:
    st.error(f"**Kh√¥ng th·ªÉ k·∫øt n·ªëi OpenAI**: {str(e)}")
    st.stop()

# ======================
# Kh·ªüi t·∫°o tin nh·∫Øn h·ªá th·ªëng
# ======================
INITIAL_SYSTEM_MESSAGE = {"role": "system", "content": rfile("01.system_trainning.txt")}
INITIAL_ASSISTANT_MESSAGE = {"role": "assistant", "content": rfile("02.assistant.txt")}

if "messages" not in st.session_state:
    st.session_state.messages = [INITIAL_SYSTEM_MESSAGE, INITIAL_ASSISTANT_MESSAGE]

# ======================
# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
# ======================
for message in st.session_state.messages:
    if message["role"] == "assistant":
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.markdown(message["content"])
    elif message["role"] == "user":
        with st.chat_message("user"):
            st.markdown(message["content"])

# ======================
# √î nh·∫≠p li·ªáu ng∆∞·ªùi d√πng
# ======================
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
    with st.chat_message("user"):
        st.markdown(prompt)

    # Th√™m v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})

    # T·∫°o ph·∫£n h·ªìi t·ª´ OpenAI v·ªõi streaming
    with st.chat_message("assistant", avatar="ü§ñ"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            stream = client.chat.completions.create(
                model=rfile("module_chatgpt.txt").strip(),
                messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
                stream=True,
                temperature=0.7,
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "‚ñå")

            message_placeholder.markdown(full_response)

        except Exception as e:
            error_msg = f"**L·ªói k·∫øt n·ªëi OpenAI**: {str(e)}"
            st.error(error_msg)
            full_response = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."

    # L∆∞u ph·∫£n h·ªìi v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})